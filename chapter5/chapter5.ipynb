{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乘法层\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # 对于乘法运算的链式传播，是将上层的局部导数乘以输入信号的反转值（对x求导->乘以输入信号y）\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "1.1 200 2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "# 买苹果\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1  # 局部导数\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dapple_price, dtax, dapple, dapple_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法层\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        # self.x = None 可省略，因为加法反向传播不需要输入信号的值\n",
    "        # self.y = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # self.x = x\n",
    "        # self.y = y\n",
    "        out = x + y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dout, dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./add_mul.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715.0000000000001, 110.00000000000001, 2.2, 165.0, 3.3000000000000003)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = 100     # 苹果单价\n",
    "apple_num = 2\n",
    "orange = 150    # 橘子单价\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)                 # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)             # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)   # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)                                     # (4)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)                           # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)   # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)             # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)                 # (1)\n",
    "\n",
    "price, dapple_num, dapple, dorange_num, dorange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1. , -0.5],\n",
       "        [-2. ,  3. ]]),\n",
       " array([[False,  True],\n",
       "        [ True, False]]),\n",
       " array([-0.5, -2. ]),\n",
       " False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask的运用\n",
    "import numpy as np\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "mask = (x <=0 )\n",
    "x, mask, x[mask], id(x) == id(x.copy())  # .copy() 会重新生成一个与原数组一摸一样的新对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU层\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[mask] = 0  # 将神经元输出小于0的值设置为0，其余保持不变\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid层\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine层 - np.dot\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)     # 对输入值求导，是因为有可能输入值来自于上一层的输出\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax层\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.y.shape[0]        # 注意这里只能处理批，不能处理单个数据\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine1 1\n",
      "Relu1 2\n",
      "1\n",
      "2\n",
      "Affine1\n",
      "Relu1\n"
     ]
    }
   ],
   "source": [
    "# OrderedDict - 有序字典\n",
    "from collections import OrderedDict\n",
    "\n",
    "od = OrderedDict()\n",
    "od[\"Affine1\"] = 1\n",
    "od[\"Relu1\"] = 2\n",
    "\n",
    "for key, val in od.items():\n",
    "    print(key, val)\n",
    "\n",
    "for val in od.values():\n",
    "    print(val)\n",
    "\n",
    "for key in od.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值微分 vs. 反向传播\n",
    "from TwoLayerNet import TwoLayerNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(one_hot_lable=True)\n",
    "\n",
    "network = TwoLayerNet(784, 50, 10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 9.636116188752832e-08\n",
      "b1: 1.896506961625321e-06\n",
      "W2: 6.379564505455199e-09\n",
      "b2: 1.4036910548631543e-07\n"
     ]
    }
   ],
   "source": [
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(f\"{key}: {diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 | train_acc: 0.095 | test_acc: 0.094\n",
      "epoch  1 | train_acc: 0.904 | test_acc: 0.907\n",
      "epoch  2 | train_acc: 0.924 | test_acc: 0.926\n",
      "epoch  3 | train_acc: 0.934 | test_acc: 0.935\n",
      "epoch  4 | train_acc: 0.944 | test_acc: 0.942\n",
      "epoch  5 | train_acc: 0.950 | test_acc: 0.948\n",
      "epoch  6 | train_acc: 0.956 | test_acc: 0.952\n",
      "epoch  7 | train_acc: 0.961 | test_acc: 0.957\n",
      "epoch  8 | train_acc: 0.964 | test_acc: 0.961\n",
      "epoch  9 | train_acc: 0.967 | test_acc: 0.962\n",
      "epoch 10 | train_acc: 0.969 | test_acc: 0.963\n",
      "epoch 11 | train_acc: 0.971 | test_acc: 0.966\n",
      "epoch 12 | train_acc: 0.972 | test_acc: 0.966\n",
      "epoch 13 | train_acc: 0.975 | test_acc: 0.969\n",
      "epoch 14 | train_acc: 0.977 | test_acc: 0.968\n",
      "epoch 15 | train_acc: 0.976 | test_acc: 0.967\n",
      "epoch 16 | train_acc: 0.978 | test_acc: 0.969\n"
     ]
    }
   ],
   "source": [
    "# 数据读入\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(one_hot_lable=True)\n",
    "# 网络参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch = 0\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 训练\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 反向传播\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    # 更新梯度\n",
    "    for key in grad.keys():\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"epoch {epoch:2} | train_acc: {train_acc:.3f} | test_acc: {test_acc:.3f}\")\n",
    "        epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
